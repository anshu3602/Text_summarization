{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extractive summarization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim==3.4.0 in c:\\users\\anshu\\anaconda3\\lib\\site-packages (3.4.0)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\anshu\\anaconda3\\lib\\site-packages (from gensim==3.4.0) (1.15.0)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in c:\\users\\anshu\\anaconda3\\lib\\site-packages (from gensim==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\anshu\\anaconda3\\lib\\site-packages (from gensim==3.4.0) (1.23.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\anshu\\anaconda3\\lib\\site-packages (from gensim==3.4.0) (1.5.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install gensim==3.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ANSHU\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ANSHU\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ANSHU\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import bs4 as bs # BeautifulSoup \n",
    "import urllib.request\n",
    "import re\n",
    "import nltk\n",
    "import collections\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer \n",
    "from nltk.tokenize import sent_tokenize \n",
    "\n",
    "from gensim.summarization.summarizer import summarize\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of the over six million articles in the english wikipedia there are some articles that wikipedians have identified as being somewhat unusual. these articles are verifiable, valuable contributions to the encyclopedia, but are a bit odd, whimsical, or something one would not expect to find in encyclop√¶dia britannica. we should take special care to meet the highest standards of an encyclopedia with these articles lest they make wikipedia appear idiosyncratic. if you wish to add an article to this list, the article in question should preferably meet one or more of these criteria:\n",
      "this definition is not precise or absolute; some articles could still be considered unusual even if they do not fit these guidelines.\n",
      "each entry on this list should be an article on its own (not merely a section in a less unusual article) and of decent quality, and in large meeting wikipedia's manual of style. for unusual contributions that are of greater levity, see wikipedia:silly things.\n",
      "in this list, a star () indicates a featured article. a plus () indicates a good article.\n",
      "see  nominative determinism for the idea that people gravitate toward careers that fit their names, e.g. urologists named splat and weedon.\n",
      "see also list of internet memes.\n",
      "wikipedia is not afraid to tackle the tough questions:\n",
      "wikipedia:featured pictures contains some unusual images.\n",
      "lawn mower racing\n",
      "train wreck at montparnasse\n",
      "the agassiz statue, stanford university, california. april 1906\n",
      "grenville diptych\n",
      "medieval trepanatio\n"
     ]
    }
   ],
   "source": [
    "# Web Scraping \n",
    "def _scrape_webpage(url): \n",
    " \n",
    "  scraped_textdata = urllib.request.urlopen(url) \n",
    "  textdata = scraped_textdata.read()\n",
    "  parsed_textdata = bs.BeautifulSoup(textdata,'lxml') \n",
    "  paragraphs = parsed_textdata.find_all('p') \n",
    "  formated_text = \"\"\n",
    "  \n",
    "  for para in paragraphs:\n",
    "    formated_text += para.text\n",
    "  return formated_text\n",
    "\n",
    "\n",
    "random_text = _scrape_webpage('https://en.wikipedia.org/wiki/Wikipedia:Unusual_articles#Physics').lower()\n",
    "print(random_text[:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process the data **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing , removing stop words , punctuation etc \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sort and group together variant forms of the same word . \n",
    "# Stemmer - process of reducing a word to its word stem that adds to suffixes and prefixes or to the roots of words\n",
    "def process(stopwords_tokens):\n",
    "  processed = []\n",
    "  stemmer = PorterStemmer()\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  for x,test_word in enumerate(stopwords_tokens):\n",
    "    word_stem = stemmer.stem(test_word)\n",
    "    word_lemmatise = lemmatizer.lemmatize(word_stem)\n",
    "    processed.append(word_lemmatise)\n",
    "  return processed\n",
    "# Here process is basically lemmatizing - grouping together , so that it can be analysed as single item "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text proceccing function\n",
    "def process_text(random_text):\n",
    "  tokenizer = RegexpTokenizer(r'[^\\d\\W]+')\n",
    "  regexp_tokens = tokenizer.tokenize(random_text.lower())\n",
    "  stopwords_tokens = [token for token in regexp_tokens if token not in stopwords.words('english')]\n",
    "  return stopwords_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call functions\n",
    "processed = process_text(random_text)\n",
    "processed = process(processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 147 samples and 184 outcomes>\n",
      "[('articl', 11), ('wikipedia', 6), ('unusu', 5), ('list', 4), ('one', 3), ('meet', 3), ('see', 3), ('million', 2), ('contribut', 2), ('encyclopedia', 2), ('question', 2), ('fit', 2), ('indic', 2), ('featur', 2), ('name', 2)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Samples', ylabel='Counts'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#List and plot the top 15 words\n",
    "freq_dist = nltk.FreqDist(processed) \n",
    "print(freq_dist)\n",
    "k = 15\n",
    "print(freq_dist.most_common(k)) \n",
    "freq_dist.plot(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sometimes we would like a word to have more weight based on its semantics. \n",
    "#In order to find the weighted mean, we need to multiply the count of words by its weight and add up the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_Weighted_frequency = pd.DataFrame(columns=[\"WORD\", \"W_FREQUENCY\"])\n",
    "\n",
    "for word, frequency in freq_dist.most_common(k):\n",
    "  DF_Weighted_frequency = DF_Weighted_frequency.append({'WORD': word, 'W_FREQUENCY': frequency}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>W_FREQUENCY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>articl</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wikipedia</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>unusu</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>list</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>one</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>meet</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>see</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>million</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>contribut</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>encyclopedia</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>question</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>fit</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>indic</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>featur</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>name</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            WORD W_FREQUENCY\n",
       "0         articl          11\n",
       "1      wikipedia           6\n",
       "2          unusu           5\n",
       "3           list           4\n",
       "4            one           3\n",
       "5           meet           3\n",
       "6            see           3\n",
       "7        million           2\n",
       "8      contribut           2\n",
       "9   encyclopedia           2\n",
       "10      question           2\n",
       "11           fit           2\n",
       "12         indic           2\n",
       "13        featur           2\n",
       "14          name           2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_Weighted_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of sentences:  15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['six',\n",
       "  'million',\n",
       "  'articl',\n",
       "  'english',\n",
       "  'wikipedia',\n",
       "  'articl',\n",
       "  'wikipedian',\n",
       "  'identifi',\n",
       "  'somewhat',\n",
       "  'unusu'],\n",
       " ['articl',\n",
       "  'verifi',\n",
       "  'valuabl',\n",
       "  'contribut',\n",
       "  'encyclopedia',\n",
       "  'bit',\n",
       "  'odd',\n",
       "  'whimsic',\n",
       "  'someth',\n",
       "  'one',\n",
       "  'would',\n",
       "  'expect',\n",
       "  'find',\n",
       "  'encyclop√¶dia',\n",
       "  'britannica'],\n",
       " ['take',\n",
       "  'special',\n",
       "  'care',\n",
       "  'meet',\n",
       "  'highest',\n",
       "  'standard',\n",
       "  'encyclopedia',\n",
       "  'articl',\n",
       "  'lest',\n",
       "  'make',\n",
       "  'wikipedia',\n",
       "  'appear',\n",
       "  'idiosyncrat'],\n",
       " ['wish',\n",
       "  'add',\n",
       "  'articl',\n",
       "  'list',\n",
       "  'articl',\n",
       "  'question',\n",
       "  'prefer',\n",
       "  'meet',\n",
       "  'one',\n",
       "  'criterion',\n",
       "  'definit',\n",
       "  'precis',\n",
       "  'absolut',\n",
       "  'articl',\n",
       "  'could',\n",
       "  'still',\n",
       "  'consid',\n",
       "  'unusu',\n",
       "  'even',\n",
       "  'fit',\n",
       "  'guidelin'],\n",
       " ['entri',\n",
       "  'list',\n",
       "  'articl',\n",
       "  'mere',\n",
       "  'section',\n",
       "  'le',\n",
       "  'unusu',\n",
       "  'articl',\n",
       "  'decent',\n",
       "  'qualiti',\n",
       "  'larg',\n",
       "  'meet',\n",
       "  'wikipedia',\n",
       "  'manual',\n",
       "  'style'],\n",
       " ['unusu',\n",
       "  'contribut',\n",
       "  'greater',\n",
       "  'leviti',\n",
       "  'see',\n",
       "  'wikipedia',\n",
       "  'silli',\n",
       "  'thing'],\n",
       " ['list', 'star', 'indic', 'featur', 'articl'],\n",
       " ['plu', 'indic', 'good', 'articl'],\n",
       " ['see',\n",
       "  'nomin',\n",
       "  'determin',\n",
       "  'idea',\n",
       "  'peopl',\n",
       "  'gravit',\n",
       "  'toward',\n",
       "  'career',\n",
       "  'fit',\n",
       "  'name',\n",
       "  'e',\n",
       "  'g'],\n",
       " ['urologist', 'name', 'splat', 'weedon'],\n",
       " ['see', 'also', 'list', 'internet', 'meme'],\n",
       " ['wikipedia',\n",
       "  'afraid',\n",
       "  'tackl',\n",
       "  'tough',\n",
       "  'question',\n",
       "  'wikipedia',\n",
       "  'featur',\n",
       "  'pictur',\n",
       "  'contain',\n",
       "  'unusu',\n",
       "  'imag'],\n",
       " ['lawn',\n",
       "  'mower',\n",
       "  'race',\n",
       "  'train',\n",
       "  'wreck',\n",
       "  'montparnass',\n",
       "  'agassiz',\n",
       "  'statu',\n",
       "  'stanford',\n",
       "  'univers',\n",
       "  'california'],\n",
       " ['april',\n",
       "  'grenvil',\n",
       "  'diptych',\n",
       "  'mediev',\n",
       "  'trepan',\n",
       "  'polydactyli',\n",
       "  'isometr',\n",
       "  'project',\n",
       "  'flaw',\n",
       "  'collaps',\n",
       "  'tacoma',\n",
       "  'narrow',\n",
       "  'bridg',\n",
       "  'defec',\n",
       "  'seagul',\n",
       "  'indec',\n",
       "  'aerial',\n",
       "  'turn',\n",
       "  'hous',\n",
       "  'tank',\n",
       "  'tread',\n",
       "  'airplan',\n",
       "  'salvador',\n",
       "  'dal√≠',\n",
       "  'mainten',\n",
       "  'mount',\n",
       "  'rushmor',\n",
       "  'one',\n",
       "  'million',\n",
       "  'color',\n",
       "  'keep',\n",
       "  'hand'],\n",
       " ['like',\n",
       "  'fli',\n",
       "  'elabor',\n",
       "  'flat',\n",
       "  'earth',\n",
       "  'map',\n",
       "  'drawn',\n",
       "  'veget',\n",
       "  'lamb',\n",
       "  'tartari',\n",
       "  'carrot',\n",
       "  'mani',\n",
       "  'color',\n",
       "  'profession',\n",
       "  'regurgit',\n",
       "  'hadji',\n",
       "  'ali',\n",
       "  'work']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sentence Tokenizing  - process of splitting text into individual sentences\n",
    "def sent_tokenize_processing(random_text):\n",
    "  processed_sent_list = []\n",
    "  sent_tokens = sent_tokenize(random_text)\n",
    "  for sentence in sent_tokens:\n",
    "    processed_sent = process_text(sentence)\n",
    "    processed_sent = process(processed_sent)\n",
    "    processed_sent_list.append(processed_sent)\n",
    "  return processed_sent_list\n",
    "\n",
    "processed_sent_list = sent_tokenize_processing(random_text)\n",
    "print(\"# of sentences: \", len(processed_sent_list)) \n",
    "processed_sent_list[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating doc_id\n",
    "\n",
    "# We need to iterate through all the words in all the document(sentences) \n",
    "# store the document id‚Äôs for each word\n",
    "#  create a set if the word doesn‚Äôt have a set yet else add to the set. \n",
    "# condition is checked by the try block\n",
    "# processed is the body of the document\n",
    "\n",
    "doc_id = {}\n",
    "for i in range(len(processed_sent_list)):\n",
    "    tokens = processed_sent_list[i]\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            doc_id[w].add(i)\n",
    "        except:\n",
    "            doc_id[w] = {i}\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'six': {0},\n",
       " 'million': {0, 13},\n",
       " 'articl': {0, 1, 2, 3, 4, 6, 7},\n",
       " 'english': {0},\n",
       " 'wikipedia': {0, 2, 4, 5, 11},\n",
       " 'wikipedian': {0},\n",
       " 'identifi': {0},\n",
       " 'somewhat': {0},\n",
       " 'unusu': {0, 3, 4, 5, 11},\n",
       " 'verifi': {1},\n",
       " 'valuabl': {1},\n",
       " 'contribut': {1, 5},\n",
       " 'encyclopedia': {1, 2},\n",
       " 'bit': {1},\n",
       " 'odd': {1},\n",
       " 'whimsic': {1},\n",
       " 'someth': {1},\n",
       " 'one': {1, 3, 13},\n",
       " 'would': {1},\n",
       " 'expect': {1},\n",
       " 'find': {1},\n",
       " 'encyclop√¶dia': {1},\n",
       " 'britannica': {1},\n",
       " 'take': {2},\n",
       " 'special': {2},\n",
       " 'care': {2},\n",
       " 'meet': {2, 3, 4},\n",
       " 'highest': {2},\n",
       " 'standard': {2},\n",
       " 'lest': {2},\n",
       " 'make': {2},\n",
       " 'appear': {2},\n",
       " 'idiosyncrat': {2},\n",
       " 'wish': {3},\n",
       " 'add': {3},\n",
       " 'list': {3, 4, 6, 10},\n",
       " 'question': {3, 11},\n",
       " 'prefer': {3},\n",
       " 'criterion': {3},\n",
       " 'definit': {3},\n",
       " 'precis': {3},\n",
       " 'absolut': {3},\n",
       " 'could': {3},\n",
       " 'still': {3},\n",
       " 'consid': {3},\n",
       " 'even': {3},\n",
       " 'fit': {3, 8},\n",
       " 'guidelin': {3},\n",
       " 'entri': {4},\n",
       " 'mere': {4},\n",
       " 'section': {4},\n",
       " 'le': {4},\n",
       " 'decent': {4},\n",
       " 'qualiti': {4},\n",
       " 'larg': {4},\n",
       " 'manual': {4},\n",
       " 'style': {4},\n",
       " 'greater': {5},\n",
       " 'leviti': {5},\n",
       " 'see': {5, 8, 10},\n",
       " 'silli': {5},\n",
       " 'thing': {5},\n",
       " 'star': {6},\n",
       " 'indic': {6, 7},\n",
       " 'featur': {6, 11},\n",
       " 'plu': {7},\n",
       " 'good': {7},\n",
       " 'nomin': {8},\n",
       " 'determin': {8},\n",
       " 'idea': {8},\n",
       " 'peopl': {8},\n",
       " 'gravit': {8},\n",
       " 'toward': {8},\n",
       " 'career': {8},\n",
       " 'name': {8, 9},\n",
       " 'e': {8},\n",
       " 'g': {8},\n",
       " 'urologist': {9},\n",
       " 'splat': {9},\n",
       " 'weedon': {9},\n",
       " 'also': {10},\n",
       " 'internet': {10},\n",
       " 'meme': {10},\n",
       " 'afraid': {11},\n",
       " 'tackl': {11},\n",
       " 'tough': {11},\n",
       " 'pictur': {11},\n",
       " 'contain': {11},\n",
       " 'imag': {11},\n",
       " 'lawn': {12},\n",
       " 'mower': {12},\n",
       " 'race': {12},\n",
       " 'train': {12},\n",
       " 'wreck': {12},\n",
       " 'montparnass': {12},\n",
       " 'agassiz': {12},\n",
       " 'statu': {12},\n",
       " 'stanford': {12},\n",
       " 'univers': {12},\n",
       " 'california': {12},\n",
       " 'april': {13},\n",
       " 'grenvil': {13},\n",
       " 'diptych': {13},\n",
       " 'mediev': {13},\n",
       " 'trepan': {13},\n",
       " 'polydactyli': {13},\n",
       " 'isometr': {13},\n",
       " 'project': {13},\n",
       " 'flaw': {13},\n",
       " 'collaps': {13},\n",
       " 'tacoma': {13},\n",
       " 'narrow': {13},\n",
       " 'bridg': {13},\n",
       " 'defec': {13},\n",
       " 'seagul': {13},\n",
       " 'indec': {13},\n",
       " 'aerial': {13},\n",
       " 'turn': {13},\n",
       " 'hous': {13},\n",
       " 'tank': {13},\n",
       " 'tread': {13},\n",
       " 'airplan': {13},\n",
       " 'salvador': {13},\n",
       " 'dal√≠': {13},\n",
       " 'mainten': {13},\n",
       " 'mount': {13},\n",
       " 'rushmor': {13},\n",
       " 'color': {13, 14},\n",
       " 'keep': {13},\n",
       " 'hand': {13},\n",
       " 'like': {14},\n",
       " 'fli': {14},\n",
       " 'elabor': {14},\n",
       " 'flat': {14},\n",
       " 'earth': {14},\n",
       " 'map': {14},\n",
       " 'drawn': {14},\n",
       " 'veget': {14},\n",
       " 'lamb': {14},\n",
       " 'tartari': {14},\n",
       " 'carrot': {14},\n",
       " 'mani': {14},\n",
       " 'profession': {14},\n",
       " 'regurgit': {14},\n",
       " 'hadji': {14},\n",
       " 'ali': {14},\n",
       " 'work': {14}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is basically then count of unique words in the document.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_id) \n",
    "#will give the unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'six': 1,\n",
       " 'million': 2,\n",
       " 'articl': 7,\n",
       " 'english': 1,\n",
       " 'wikipedia': 5,\n",
       " 'wikipedian': 1,\n",
       " 'identifi': 1,\n",
       " 'somewhat': 1,\n",
       " 'unusu': 5,\n",
       " 'verifi': 1,\n",
       " 'valuabl': 1,\n",
       " 'contribut': 2,\n",
       " 'encyclopedia': 2,\n",
       " 'bit': 1,\n",
       " 'odd': 1,\n",
       " 'whimsic': 1,\n",
       " 'someth': 1,\n",
       " 'one': 3,\n",
       " 'would': 1,\n",
       " 'expect': 1,\n",
       " 'find': 1,\n",
       " 'encyclop√¶dia': 1,\n",
       " 'britannica': 1,\n",
       " 'take': 1,\n",
       " 'special': 1,\n",
       " 'care': 1,\n",
       " 'meet': 3,\n",
       " 'highest': 1,\n",
       " 'standard': 1,\n",
       " 'lest': 1,\n",
       " 'make': 1,\n",
       " 'appear': 1,\n",
       " 'idiosyncrat': 1,\n",
       " 'wish': 1,\n",
       " 'add': 1,\n",
       " 'list': 4,\n",
       " 'question': 2,\n",
       " 'prefer': 1,\n",
       " 'criterion': 1,\n",
       " 'definit': 1,\n",
       " 'precis': 1,\n",
       " 'absolut': 1,\n",
       " 'could': 1,\n",
       " 'still': 1,\n",
       " 'consid': 1,\n",
       " 'even': 1,\n",
       " 'fit': 2,\n",
       " 'guidelin': 1,\n",
       " 'entri': 1,\n",
       " 'mere': 1,\n",
       " 'section': 1,\n",
       " 'le': 1,\n",
       " 'decent': 1,\n",
       " 'qualiti': 1,\n",
       " 'larg': 1,\n",
       " 'manual': 1,\n",
       " 'style': 1,\n",
       " 'greater': 1,\n",
       " 'leviti': 1,\n",
       " 'see': 3,\n",
       " 'silli': 1,\n",
       " 'thing': 1,\n",
       " 'star': 1,\n",
       " 'indic': 2,\n",
       " 'featur': 2,\n",
       " 'plu': 1,\n",
       " 'good': 1,\n",
       " 'nomin': 1,\n",
       " 'determin': 1,\n",
       " 'idea': 1,\n",
       " 'peopl': 1,\n",
       " 'gravit': 1,\n",
       " 'toward': 1,\n",
       " 'career': 1,\n",
       " 'name': 2,\n",
       " 'e': 1,\n",
       " 'g': 1,\n",
       " 'urologist': 1,\n",
       " 'splat': 1,\n",
       " 'weedon': 1,\n",
       " 'also': 1,\n",
       " 'internet': 1,\n",
       " 'meme': 1,\n",
       " 'afraid': 1,\n",
       " 'tackl': 1,\n",
       " 'tough': 1,\n",
       " 'pictur': 1,\n",
       " 'contain': 1,\n",
       " 'imag': 1,\n",
       " 'lawn': 1,\n",
       " 'mower': 1,\n",
       " 'race': 1,\n",
       " 'train': 1,\n",
       " 'wreck': 1,\n",
       " 'montparnass': 1,\n",
       " 'agassiz': 1,\n",
       " 'statu': 1,\n",
       " 'stanford': 1,\n",
       " 'univers': 1,\n",
       " 'california': 1,\n",
       " 'april': 1,\n",
       " 'grenvil': 1,\n",
       " 'diptych': 1,\n",
       " 'mediev': 1,\n",
       " 'trepan': 1,\n",
       " 'polydactyli': 1,\n",
       " 'isometr': 1,\n",
       " 'project': 1,\n",
       " 'flaw': 1,\n",
       " 'collaps': 1,\n",
       " 'tacoma': 1,\n",
       " 'narrow': 1,\n",
       " 'bridg': 1,\n",
       " 'defec': 1,\n",
       " 'seagul': 1,\n",
       " 'indec': 1,\n",
       " 'aerial': 1,\n",
       " 'turn': 1,\n",
       " 'hous': 1,\n",
       " 'tank': 1,\n",
       " 'tread': 1,\n",
       " 'airplan': 1,\n",
       " 'salvador': 1,\n",
       " 'dal√≠': 1,\n",
       " 'mainten': 1,\n",
       " 'mount': 1,\n",
       " 'rushmor': 1,\n",
       " 'color': 2,\n",
       " 'keep': 1,\n",
       " 'hand': 1,\n",
       " 'like': 1,\n",
       " 'fli': 1,\n",
       " 'elabor': 1,\n",
       " 'flat': 1,\n",
       " 'earth': 1,\n",
       " 'map': 1,\n",
       " 'drawn': 1,\n",
       " 'veget': 1,\n",
       " 'lamb': 1,\n",
       " 'tartari': 1,\n",
       " 'carrot': 1,\n",
       " 'mani': 1,\n",
       " 'profession': 1,\n",
       " 'regurgit': 1,\n",
       " 'hadji': 1,\n",
       " 'ali': 1,\n",
       " 'work': 1}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# doc_id will have the word as the key and list of document id‚Äôs as the value\n",
    "# for doc_id we don‚Äôt actually need the list of docs, we just need the count. \n",
    "# so we are going to replace the list with its count.\n",
    "for i in doc_id:\n",
    "  doc_id[i] = len(doc_id[i])\n",
    "doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['six', 'million', 'articl', 'english', 'wikipedia', 'wikipedian', 'identifi', 'somewhat', 'unusu', 'verifi', 'valuabl', 'contribut', 'encyclopedia', 'bit', 'odd', 'whimsic', 'someth', 'one', 'would', 'expect', 'find', 'encyclop√¶dia', 'britannica', 'take', 'special', 'care', 'meet', 'highest', 'standard', 'lest', 'make', 'appear', 'idiosyncrat', 'wish', 'add', 'list', 'question', 'prefer', 'criterion', 'definit', 'precis', 'absolut', 'could', 'still', 'consid', 'even', 'fit', 'guidelin', 'entri', 'mere', 'section', 'le', 'decent', 'qualiti', 'larg', 'manual', 'style', 'greater', 'leviti', 'see', 'silli', 'thing', 'star', 'indic', 'featur', 'plu', 'good', 'nomin', 'determin', 'idea', 'peopl', 'gravit', 'toward', 'career', 'name', 'e', 'g', 'urologist', 'splat', 'weedon', 'also', 'internet', 'meme', 'afraid', 'tackl', 'tough', 'pictur', 'contain', 'imag', 'lawn', 'mower', 'race', 'train', 'wreck', 'montparnass', 'agassiz', 'statu', 'stanford', 'univers', 'california', 'april', 'grenvil', 'diptych', 'mediev', 'trepan', 'polydactyli', 'isometr', 'project', 'flaw', 'collaps', 'tacoma', 'narrow', 'bridg', 'defec', 'seagul', 'indec', 'aerial', 'turn', 'hous', 'tank', 'tread', 'airplan', 'salvador', 'dal√≠', 'mainten', 'mount', 'rushmor', 'color', 'keep', 'hand', 'like', 'fli', 'elabor', 'flat', 'earth', 'map', 'drawn', 'veget', 'lamb', 'tartari', 'carrot', 'mani', 'profession', 'regurgit', 'hadji', 'ali', 'work']\n"
     ]
    }
   ],
   "source": [
    "# Unique words\n",
    "total_unique = [x for x in doc_id]\n",
    "print(total_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_idf - frequency‚Äìinverse document frequency\n",
    "#  numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus\n",
    "\n",
    "#Calculating td_idf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of sentences (documents)\n",
    "len(processed_sent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating tf_idf\n",
    "#math formula\n",
    "#tf(t,d) = count of t in d / number of words in d\n",
    "#Document Frequency\n",
    "#df(t) = occurrence of t in documents\n",
    "#Inverse Document Frequency\n",
    "#idf(t) = N/df\n",
    "\n",
    "\n",
    "# Creating an empty Dataframe with column names only\n",
    "df_tf_idf = pd.DataFrame(columns=['WORD', 'TF_IDF'])\n",
    "\n",
    "\n",
    "N = len(processed_sent_list)\n",
    "tf_idf = {}\n",
    "thisdict = {}\n",
    "for i in range(N):\n",
    "  tokens = processed_sent_list[i]\n",
    "  counter = collections.Counter(tokens)\n",
    "  len(processed_sent_list[i])\n",
    "  for token in np.unique(tokens):\n",
    "    #This measures the frequency of a word in a document\n",
    "    tf = counter[token]/len(processed_sent_list[i])\n",
    "    df = doc_id[token]\n",
    "    #IDF is the inverse of the document frequency which measures the informativeness of term t\n",
    "    idf = np.log(N/(df+1)) #As we cannot divide by 0, we smoothen the value by adding 1 to the denominator\n",
    "    #Finally, by taking a multiplicative value of TF and IDF, we get the TF-IDF score\n",
    "    tf_idf[token] = tf*idf\n",
    "\n",
    "    thisdict.update( {token : tf_idf[token]} )\n",
    "    df_tf_idf =  df_tf_idf.append({'WORD': token, 'TF_IDF': tf_idf[token]}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>TF_IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>articl</td>\n",
       "      <td>0.125722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>english</td>\n",
       "      <td>0.201490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>identifi</td>\n",
       "      <td>0.201490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>million</td>\n",
       "      <td>0.160944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>six</td>\n",
       "      <td>0.201490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>profession</td>\n",
       "      <td>0.111939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>regurgit</td>\n",
       "      <td>0.111939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>tartari</td>\n",
       "      <td>0.111939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>veget</td>\n",
       "      <td>0.111939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>work</td>\n",
       "      <td>0.111939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>179 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           WORD    TF_IDF\n",
       "0        articl  0.125722\n",
       "1       english  0.201490\n",
       "2      identifi  0.201490\n",
       "3       million  0.160944\n",
       "4           six  0.201490\n",
       "..          ...       ...\n",
       "174  profession  0.111939\n",
       "175    regurgit  0.111939\n",
       "176     tartari  0.111939\n",
       "177       veget  0.111939\n",
       "178        work  0.111939\n",
       "\n",
       "[179 rows x 2 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf(t,d) = count of t in d / number of words in d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a summary based on ratio, sentence or word count , etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of the over six million articles in the english wikipedia there are some articles that wikipedians have identified as being somewhat unusual.\n",
      "each entry on this list should be an article on its own (not merely a section in a less unusual article) and of decent quality, and in large meeting wikipedia's manual of style.\n",
      "in this list, a star () indicates a featured article.\n"
     ]
    }
   ],
   "source": [
    "#Summarize based on ratio\n",
    "print(summarize(random_text, ratio=0.10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "each entry on this list should be an article on its own (not merely a section in a less unusual article) and of decent quality, and in large meeting wikipedia's manual of style.\n"
     ]
    }
   ],
   "source": [
    "#Summarize based on word count\n",
    "print(summarize(random_text, word_count=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarize the same text data using Gensim with TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "of the over six million articles in the english wikipedia there are some articles that wikipedians have identified as being somewhat unusual.\n",
      "each entry on this list should be an article on its own (not merely a section in a less unusual article) and of decent quality, and in large meeting wikipedia's manual of style.\n",
      "in this list, a star () indicates a featured article.\n",
      "\n",
      "Keywords:\n",
      "   Gensim_Text_Rank\n",
      "0         wikipedia\n",
      "1           unusual\n",
      "2          articles\n",
      "3           article\n",
      "4              meet\n",
      "5           meeting\n",
      "6             names\n",
      "7             named\n",
      "8               odd\n",
      "9         indicates\n",
      "10         featured\n",
      "11             dali\n",
      "12         salvador\n",
      "13   flat earth map\n",
      "14             tank\n",
      "15          narrows\n",
      "16           statue\n",
      "17             flaw\n",
      "18            train\n",
      "19            hadji\n",
      "20         valuable\n"
     ]
    }
   ],
   "source": [
    "from gensim.summarization import keywords\n",
    "\n",
    "\n",
    "print('Summary:')\n",
    "print(summarize(random_text, ratio=0.10))\n",
    "\n",
    "df_Gensim = pd.DataFrame(columns=[\"Gensim_Text_Rank\"])\n",
    "df_Gensim['Gensim_Text_Rank'] = keywords(random_text, split=True)\n",
    "\n",
    "print('\\nKeywords:')\n",
    "print(df_Gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the Natural Language Toolkit to process the text information and regular expressions to preprocess the data\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from collections import defaultdict\n",
    "\n",
    "import bs4 as BS\n",
    "import urllib.request\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of the over six million articles in the english wikipedia there are some articles that wikipedians have identified as being somewhat unusual. these articles are verifiable, valuable contributions to the encyclopedia, but are a bit odd, whimsical, or something one would not expect to find in encyclop√¶dia britannica. we should take special care to meet the highest standards of an encyclopedia with these articles lest they make wikipedia appear idiosyncratic. if you wish to add an article to this list, the article in question should preferably meet one or more of these criteria:\n",
      "this definition is not precise or absolute; some articles could still be considered unusual even if they do not fit these guidelines.\n",
      "each entry on this list should be an article on its own (not merely a section in a less unusual article) and of decent quality, and in large meeting wikipedia's manual of style. for unusual contributions that are of greater levity, see wikipedia:silly things.\n",
      "in this list, a star () indicates a featured article. a plus () indicates a good article.\n",
      "see  nominative determinism for the idea that people gravitate toward careers that fit their names, e.g. urologists named splat and weedon.\n",
      "see also list of internet memes.\n",
      "wikipedia is not afraid to tackle the tough questions:\n",
      "wikipedia:featured pictures contains some unusual images.\n",
      "lawn mower racing\n",
      "train wreck at montparnasse\n",
      "the agassiz statue, stanford university, california. april 1906\n",
      "grenville diptych\n",
      "medieval trepanation\n",
      "polydactyly\n",
      "isometric projection flaw\n",
      "collapse of the tacoma narrows bridge\n",
      "defecating seagull\n",
      "indecency\n",
      "aerial turning house\n",
      "tank treads on an airplane\n",
      "salvador dal√≠\n",
      "maintenance of mount rushmore\n",
      "one million colors\n",
      "keep your hands to yourself!\n",
      "like a fly on...\n",
      "an elaborate flat earth map drawn in 1893\n",
      "vegetable lamb of tartary\n",
      "carrots of many colors\n",
      "professional regurgitator hadji ali at work\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#parsing the data \n",
    "data_parsed = BS.BeautifulSoup(random_text,'html.parser')\n",
    "\n",
    "print(data_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dictionary table of weighted frequencies of words\n",
    "\n",
    "def create_freq_dictionary(content):\n",
    "    \n",
    "    #stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    words = word_tokenize(content)\n",
    "    \n",
    "    #Instantiate stemmer \n",
    "    # Stemming refers to the removal of any suffixes (and sometimes other affixes) from an input word to produce a stem\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    freq_dict = defaultdict(int)\n",
    "    \n",
    "    weighted_freq_dict = dict()\n",
    "    \n",
    "    #Build the dictionary by looping through the words\n",
    "    \n",
    "    for word in words:\n",
    "        \n",
    "        if word in stop_words:\n",
    "            continue\n",
    "        freq_dict[word]+=1\n",
    "        \n",
    "    #Find the maximum value of frequencies to find the weighted values\n",
    "    dict_values = freq_dict.values()\n",
    "    \n",
    "    max_value = max(dict_values)\n",
    "    \n",
    "    for word in freq_dict.keys():\n",
    "        weighted_freq_dict[word] = freq_dict[word]/max_value\n",
    "        \n",
    "    return weighted_freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate scores for each sentence \n",
    "\n",
    "def get_sentence_scores(sentences,weighted_word_frequency_dict):\n",
    "    \n",
    "    sentence_score = defaultdict(int)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_length = 0\n",
    "        \n",
    "        for word in weighted_word_frequency_dict:\n",
    "            if word in sentence.lower():\n",
    "                sentence_length = sentence_length + 1\n",
    "                \n",
    "                sentence_score[sentence[:10]] += weighted_word_frequency_dict[word]\n",
    "        \n",
    "        #Normalize the sentence weight by the length of the sentence\n",
    "        sentence_score[sentence[:10]] = sentence_score[sentence[:10]]/sentence_length\n",
    "    \n",
    "    return sentence_score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the threshold value = Average sentence score\n",
    "\n",
    "def get_average_sentence_score(sentence_score):\n",
    "    \n",
    "    sum_scores = 0 \n",
    "    \n",
    "    for sent , score in sentence_score.items():\n",
    "        sum_scores += score\n",
    "        \n",
    "    average_score = sum_scores/len(sentence_score)\n",
    "    \n",
    "    return average_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Summary\n",
    "\n",
    "def get_summary_sentences(sentences,sentence_score,threshold):\n",
    "    \n",
    "    num_sentences = 0\n",
    "    \n",
    "    summary = ''\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if sentence[:10] in sentence_score:\n",
    "            if sentence_score[sentence[:10]] >= threshold:\n",
    "                num_sentences = num_sentences + 1\n",
    "                summary += sentence\n",
    "                \n",
    "    print(\"The number of sentences in the summary : \",num_sentences)\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the summary by calling the above utility functions\n",
    "def generate_summary(content):\n",
    "    \n",
    "    #Build weighted dictionary of words\n",
    "    weighted_word_frequency_dict = create_freq_dictionary(content)\n",
    "    \n",
    "    #Tokenize content into sentences\n",
    "    \n",
    "    sentences = sent_tokenize(content)\n",
    "    \n",
    "    print(\"The number of sentences in the original article : \" , len(sentences))\n",
    "    \n",
    "    #Compute sentence_scores\n",
    "    \n",
    "    sentence_scores = get_sentence_scores(sentences,weighted_word_frequency_dict)\n",
    "    \n",
    "    #Get the threshold score\n",
    "    \n",
    "    threshold = get_average_sentence_score(sentence_scores)\n",
    "    \n",
    "    #Create the summary\n",
    "    \n",
    "    summary = get_summary_sentences(sentences,sentence_scores,threshold)\n",
    "    \n",
    "    return summary\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of sentences in the original article :  15\n",
      "The number of sentences in the summary :  6\n",
      "Generated Summary : \n",
      "of the over six million articles in the english wikipedia there are some articles that wikipedians have identified as being somewhat unusual.for unusual contributions that are of greater levity, see wikipedia:silly things.in this list, a star () indicates a featured article.a plus () indicates a good article.urologists named splat and weedon.see also list of internet memes.\n"
     ]
    }
   ],
   "source": [
    "#Main function \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    summary = generate_summary(random_text)\n",
    "    \n",
    "    print(\"Generated Summary : \")\n",
    "    \n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
